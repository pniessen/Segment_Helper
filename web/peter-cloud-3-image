# Rosetta Consulting customer segmentation pipeline
# 1.20.2016
# Peter Niessen

import numpy as np
from sklearn.decomposition import PCA
import time
import pandas as pd
import flask
import os
from flask import Flask, request, jsonify, render_template, send_from_directory, url_for
from werkzeug import secure_filename
from collections import Counter, OrderedDict
import pprint
from numpy import loadtxt
from joblib import Parallel, delayed
import multiprocessing
import random
import xlsxwriter
import sys
import itertools

arguments = sys.argv[1:]
print "Command line arguments:", str(arguments)

interactive_mode = False
xls = False

if ('-i' or '-I') in arguments:
	interactive_mode = True

if ('-xls' or '-xl' or '-x') in arguments:
	xls = 'True'

app = Flask(__name__)
start_time = time.time()

basedir_for_upload = os.path.abspath(os.path.dirname(__file__))
basedir = os.path.abspath(os.path.dirname(__file__)) + "/"
print basedir_for_upload

from logging import Formatter, FileHandler
handler = FileHandler(os.path.join(basedir_for_upload, 'log.txt'), encoding='utf8')
handler.setFormatter(
    Formatter("[%(asctime)s] %(levelname)-8s %(message)s", "%Y-%m-%d %H:%M:%S")
)
app.logger.addHandler(handler)

# These are the extension that we are accepting to be uploaded
app.config['ALLOWED_EXTENSIONS'] = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif', 'csv', 'xls'])

# Step 1: load datasets, namess
# load datafile

def get_PCA(filename, n_components):
	# VAR (question name), Label (full question), Dimension (freetext label), Final Load (=max PCA value), Primary Factor (if diff), 2nd Load, Bucket Scheme., T%, M%, B%, RH, Run: [11, 12, 13, 14]
	filename, n_components = filename, n_components
	
	names = list(np.genfromtxt(basedir+filename, delimiter=',', names=True).dtype.names)
	question_dict = {}
	for question_name in names:
		question_dict[question_name] = {}
		question_dict[question_name]['question_text'] = "n/a"
		question_dict[question_name]['dimension'] = "n/a"
	print "question_dict initialized with ", len(question_dict.keys()), " questions"

	print names
	global X
	X = np.genfromtxt(basedir+filename, delimiter=',', skip_header=1)

	print filename, "loaded"
	print X.shape
	print X[0] # print one row
	#print X.dtype.names

	# transpose to get features on X axis and respondents on Y ax
	#X = X.T
	#print X.shape

	# sameple array for testing
	#X = np.array([[-1, -1, 2, 5], [-2, -1, 5, 7], [-3, -2, 6, 1], [1, 1, 4, 5], [2, 1, 5, 9], [3, 2, 6, 5]])

	# step 2: Principal Components Analysis

	pca = PCA(n_components=n_components)
	pca.fit(X)

	print "number of components:" , pca.n_components_
	print("Runtime: %s seconds ---" % (time.time() - start_time))
	#print pca.components_.shape

	# transpose to get compontents on X axis and questions on Y
	global factor_matrix
	factor_matrix = pca.components_.T
	print factor_matrix.shape
	print "sample row: ", factor_matrix[0]
	print "max absolute value:", np.absolute(factor_matrix)[0].max()

	return factor_matrix, names, X, question_dict

def top_n_factors (factor_matrix, top_n, question_dict):
	# VAR (question name), Label (full question), Dimension (freetext label), Final Load (=max PCA value), Primary Factor (if diff), 2nd Load, Bucket Scheme., T%, M%, B%, RH, Run: [11, 12, 13, 14]
	factor_matrix, n, question_dict = factor_matrix, top_n, question_dict
	# Step 3: identify largest PC factor for each question
	# add two rows of zeros for top 2 factors
	# np.append(a, z, axis=1)
	# stackoverflow.com/questions/8486294/how-to-add-an-extra-column-to-an-numpy-array

	# add extra cols for row (=question) index, #1 factor, #2 factor, best_rh
	factor_matrix_size = factor_matrix.shape # = C x R
	print factor_matrix_size
	num_rows = factor_matrix_size[0]
	num_cols = factor_matrix_size[1]
	print "number of rows: " , num_rows
	print "nunber of columns: ", num_cols
	question_number = num_cols
	best_factor = num_cols + 1
	second_best_factor = num_cols + 2
	rh = num_cols + 3

	zero_matrix = np.zeros((num_rows,4), dtype=np.int64)
	print zero_matrix.shape

	factor_matrix = np.append(factor_matrix, zero_matrix, axis=1)
	print factor_matrix.shape
	#print "sample row: ", factor_matrix[0]
	#print "sample of last four cells in row", factor_matrix[0][num_cols:]

	for row in range(num_rows):
		#row = 0
		one_question_vector = np.absolute(factor_matrix[row])
		#print "before:", one_question_vector
		#print "max_value: ", one_question_vector.max()
		#print "max value is with component number", one_question_vector.tolist().index(one_question_vector.max())+1

		# find top n factors for each question
		n=2
		top_n_factors = one_question_vector.argsort()[-n:][::-1] #[::-1] to sort H-L, returns index (=component number) of n highest factors
		#print "top 2 factors:", top_n_factors + 1 # add +1 because top_n_factors is index (first value = 0)	
		top_n_factor_values = one_question_vector[top_n_factors] # use index to find actual values
		#print "top 2 factor values: ", top_n_factor_values

		# add back to factor_matrix
		factor_matrix[row][question_number] = row # index for question
		factor_matrix[row][best_factor], factor_matrix[row][second_best_factor] = top_n_factors
		question_dict[names[row]]['first_factor'], question_dict[names[row]]['second_factor'] = top_n_factors
		question_dict[names[row]]['first_factor_value'], question_dict[names[row]]['second_factor_value'] = top_n_factor_values

		#print "after: ", factor_matrix[row]

		# check to see that successfully inserted back into factor_matrix
		if not (factor_matrix[row][best_factor:second_best_factor+1]  == top_n_factors).all():
			print "error in row", row
			print factor_matrix[row][best_factor:second_best_factor+1]
			print top_n_factors

	print "found top 2 factors for", num_rows, " questions (=rows)"
	#print question_dict

	print("Runtime: %s seconds ---" % (time.time() - start_time))

	return factor_matrix, num_rows, num_cols, question_number, rh, best_factor, second_best_factor, question_dict

def rebucket(factor_matrix, names, X, rh, question_dict):
	factor_matrix, names, X, rh, question_dict = factor_matrix, names, X, rh, question_dict
	# Step 4: calulate Rosetta Heuristic, identify optimal bucketing scheme, rebucket response matrix
	# shape = row x columns
	# get a row of X (= one set of responses to all questions from a single respondee)
	# print X[0]
	# get a column of X (= one set of responses from all respondeees to a single question)
	# print X[:,0]
	best_schemes = []

	for column in range(X.shape[1]):
		one_column = X[:,column]

		from collections import Counter
		responses = [1,2,3,4,5,6,7]
		# how to handle alternative response ranges (binary, 1-5, etc)
		response_counts = [(Counter(one_column))[r] for r in responses]
		#print response_counts
		response_shares = [float(response_counts[x]) / sum(response_counts) for x in range(len(response_counts))]
		#print response_shares

		# Mapping heuristic: XYZ: (% top X boxes - .26) + (% bottom Z boxes - .26) + (% middle Y boxes - .48) + (% top X boxes - + % bottom Z boxes)
		bucket_schemes = [(2,3,2),(3,2,2), (2,2,3), (3,1,3), (1,3,3), (1,2,4)]
		rosetta_heuristics = []

		# next step: for col in colums, or wrap as function then use map() to apply to each column?
		for bucket_scheme in bucket_schemes:
		    top_bucket = sum(response_shares[0:bucket_scheme[0]])
		    #print top_bucket
		    middle_bucket = sum(response_shares[bucket_scheme[0]:bucket_scheme[0]+ bucket_scheme[1]])
		    #print middle_bucket
		    bottom_bucket = sum(response_shares[bucket_scheme[0]+ bucket_scheme[1]:bucket_scheme[0]+ bucket_scheme[1] + bucket_scheme[2]])
		    #bottom_bucket = 1 - top_bucket - middle_bucket
		    #print bottom_bucket
		    #if not top_bucket + middle_bucket + bottom_bucket == 1:
		    #	print "bucket weights wrong!"
		    #	print float(top_bucket + middle_bucket + bottom_bucket)
		    rosetta_heuristic = np.absolute((top_bucket - .26)) + np.absolute((bottom_bucket - .26)) + np.absolute((middle_bucket - .48)) + np.absolute((top_bucket - bottom_bucket)) * 100
		    #print "RH:", rosetta_heuristic
		    rosetta_heuristics.append(rosetta_heuristic)
		
		best_rh = min(rosetta_heuristics)
		best_scheme = rosetta_heuristics.index(best_rh)
		question_dict[names[column]]['best_rh'] = best_rh
		question_dict[names[column]]['bucket_scheme'] = bucket_schemes[best_scheme]
		best_schemes.append(bucket_schemes[best_scheme])
		factor_matrix[column][rh] = best_rh
		#print "after after: ", factor_matrix[column]
		#print column, " :", bucket_schemes[best_scheme], "Best RH:", min(rosetta_heuristics)
	print best_schemes[:10]

	# now rebucket response matrix
	# could this all be redone using map()?

	X_rebucketed = np.zeros((X.shape), dtype=np.int64)
	for col in range(X_rebucketed.shape[1]):
		mapping_scheme = reduce(lambda x,y: x+y,[a*[b] for a,b in zip(best_schemes[int(col)],[1,2,3])])
		#mapping_scheme = [x+y for x,y in a*[b] for a,b in [zip(best_schemes[int(col)],[1,2,3])]]
		#X_rebucketed[:,col] = map(lambda x: mapping_scheme[int(x)-1], X[:,col])
		X_rebucketed[:,col] = [mapping_scheme[int(x)-1] for x in X[:,col]]
		# http://stackoverflow.com/questions/3459098/create-list-of-single-item-repeated-n-times-in-python
		# http://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python
		rebucket_counts = Counter(X_rebucketed[:,col]).values()
		question_dict[names[col]]['rebucket_counts_1'], question_dict[names[col]]['rebucket_counts_2'], question_dict[names[col]]['rebucket_counts_3'] = rebucket_counts
		question_dict[names[col]]['rebucket_shares_1'], question_dict[names[col]]['rebucket_shares_2'], question_dict[names[col]]['rebucket_shares_3'] = [float(value) / sum(rebucket_counts) for value in rebucket_counts]

	X_rebucketed_df = pd.DataFrame(X_rebucketed, columns=names)

	# rebucketed_filename = 'X_rebucketed.csv'
	# np.savetxt(basedir + rebucketed_filename, X_rebucketed, fmt='%.2f', delimiter=",")
	# print rebucketed_filename, "saved to:", basedir

	rebucketed_filename = 'X_rebucketed.csv'
	X_rebucketed_df.to_csv(basedir + rebucketed_filename, index=False)
	print rebucketed_filename, "saved to:", basedir

	#http://stackoverflow.com/questions/7701429/efficient-evaluation-of-a-function-at-every-cell-of-a-numpy-array
	#def rebucket(x):
	#	return mapping_scheme[(int(x))-1]
	#
	#rebucket_v = np.vectorize(rebucket)  # or use a different name if you want to keep the original f
	#X_rebucketed_2 = rebucket_v(X) 

	print "rebucketing check:"
	print "response matrix rebucketed (row, column):"
	print X_rebucketed.shape
	random_col = np.random.randint(0,X.shape[1])
	print "original responses:", X[:,random_col]
	print "bucketing scheme: ", best_schemes[random_col]
	print "rebucketed responses:", X_rebucketed[:,random_col]

	# print question_dict

	return rebucketed_filename, X_rebucketed, question_dict

def cluster_seed(factor_matrix, best_factor, question_number, num_cols, num_rows):
	# Step 5: now group questions by highest factor (=factor_matrix[best_factor], remember col #1 = index 0!)
	# http://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-column
	
	factor_matrix, best_factor, question_number, num_cols, num_rows = factor_matrix, best_factor, question_number, num_cols, num_rows

	#print factor_matrix.shape
	sorted_factor_matrix = factor_matrix[factor_matrix[:,best_factor].argsort()]
	#print "verify question index:", sorted_factor_matrix[:,question_number]
	#print "verify sort by highest factor", sorted_factor_matrix[:,best_factor]

	# determine unique factor numbers:
	unique_factor_list= list(set(sorted_factor_matrix[:,num_cols+1]))
	# print unique_factor_list

	# now group questions by primary factor and sort by RH to form cluster 'seed'
	cluster_seed=[]
	# print ("[question number, index value, rosetta heuristic]")
	for factor in unique_factor_list:
		# filter by primary factor, returns question indices, then list questions 
		question_index = sorted_factor_matrix[sorted_factor_matrix[:,best_factor]==factor][:,question_number] # filter == factor
		rh_list = sorted_factor_matrix[sorted_factor_matrix[:,best_factor]==factor][:,rh] # filter == factor
		question_index_list = list(question_index) # convert to list from np.array
		question_index_int_list = [int(x) for x in question_index_list] # convert from dtype = numpy.64 to int
		question_names = [names[q] for q in question_index_int_list] # finally filter question list by question index
		rh_list = list(rh_list)
		rh_list = [round(x) for x in rh_list]
		#print int(factor), ":", [names[q] for q in question_index_int_list] # finally filter question list by question index
		#print rh_list
		question_index_rh_list = sorted(zip(question_names, question_index_list, rh_list), key = lambda x: x[2])
		# print int(factor+1), ":", question_index_rh_list #this prints sorted RH by question
		cluster_seed.append(question_index_rh_list[0][1])

	print "cluster seed:", cluster_seed
	print "number of items in cluster seed", len(cluster_seed)

	return cluster_seed

# step 6: use cluster_seed as input to poLCA
#def poLCA(cluster_seed, num_seg, num_rep, rebucketed_filename):
def poLCA(i, cluster_seed, num_seg, num_rep, rebucketed_filename):
	import subprocess
	import base64
	import uuid

	#return 


	# cluster_seed, num_seg, num_rep, rebucketed_filename = cluster_seed, num_seg, num_rep, rebucketed_filename

	# # number of segments
	#num_seg = 6
	#num_rep = 1

	# # Define command and arguments
	command = 'Rscript'
	# #path2script = '/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/poLCA_test_v2.R'
	# path2script = '/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/simple_poLCA_test.R'
	path2script = basedir +'simple_poLCA_test.R'
	#rebucketed_filename = "X_rebucketed.csv"
	infile = basedir + rebucketed_filename

	#cluster_seed_names = ['q06_1', 'q06_2', 'q06_3','q06_4','q06_5','q06_6','q06_7']

	# # Variable number of args in a list
	# #args = ['A', 'B', 'C', 'D']
	# #args = ['11', '3', '9', '42']
	cluster_seed_names = [names[int(value)] for value in cluster_seed]
	#timestamp = base64.b64encode(str(time.time() + np.random.random_integers(0,1000000000000)))
	timestamp = str(uuid.uuid4()) # http://stackoverflow.com/questions/534839/how-to-create-a-guid-in-python
	# Q: how is names() global?
	cluster_seeds = [num_seg] + [num_rep] + [basedir] + [rebucketed_filename] + [timestamp] + cluster_seed_names
	cluster_seeds = [str(seed) for seed in cluster_seeds]
	#print cluster_seeds
	# #args = ['74','68','75','73','162','182','168','69','78','70','113','181','179','81','201']
	args = cluster_seeds
	# #print args
	# # Build subprocess command
	cmd = [command, path2script] + args

	#print cmd

	# # check_output will run the command and store to result
	print "Running poLCA in R, please wait:"
	x = subprocess.check_output(cmd, universal_newlines=True)

	#return
	#scorecard(cluster_seeds, cluster_seed_names, num_seg)
	print("------- Runtime: %.2f seconds -------" % (time.time() - start_time))
	return timestamp, cluster_seeds, cluster_seed_names, num_seg

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']

@app.route("/")
def hello():
    #return "Hello World!"
    #"/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/startbootstrap-grayscale-1.0.6/index.html"
    #"/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/table.html"
    with open(basedir + 'index.html', 'r') as viz_file:
        return viz_file.read()

@app.route("/data", methods=["GET"])
def scorecard():

	print "in scorecard function"
	training_list = []

	print "results_dict size: ", len(results_dict.keys())
	print "results_dict keys: ", results_dict.keys()

	for key in results_dict:
		#train_dict = {'a':key,'b':results_dict[key]['cluster_number'],'c':results_dict[key]['num_vars'],'d':esults_dict[key]['model_stats'],'e':16,'f':17}
		train_dict = {'a':key[:10],'b':results_dict[key]['cluster_number'],'c':results_dict[key]['num_vars'],'d':round(results_dict[key]['model_stats'][0]),'e':round(results_dict[key]['model_stats'][1]),'f':round(results_dict[key]['model_stats'][2]), 'g':round(results_dict[key]['weighted_average_cluster_polarity'],4) ,'h':round(results_dict[key]['average_cross_question_polarity'],4)}
		
		#print "one train dict entry:", train_dict
		#sorted(d, key=d.get)

		# print key
		# print "number of clusters:", results_dict[key]['cluster_number'] 
		# print "number of variables", results_dict[key]['num_vars'] 
		# print "model stats:", results_dict[key]['model_stats']
		# print "weighted average cluster polarity: ", results_dict[key]['weighted_average_cluster_polarity']
		# print "average cross-question polarity: ", results_dict[key]['average_cross_question_polarity']

		training_list.append(train_dict)
		#print training_list

	training_results = {'training': training_list}

	return flask.jsonify(training_results)

@app.route("/tracker", methods=["GET"])
def run_tracker():

	tracker_list = []
	print "in run_tracker function"
	#print question_dict.keys()

	#question_dict[question]['run_tracker']
	for key in question_dict:
		r_list = ['R'] * len(results_dict.keys())
		run_num_list = range(len(results_dict.keys()))
		r_dict_keys = [one_r + str(one_run).zfill(3) for one_r, one_run in zip(r_list,run_num_list)]
		r_dict = OrderedDict(zip(r_dict_keys, question_dict[key]['bool_run_tracker']))
		#r_dict = OrderedDict(sorted(r_dict.items(), key=lambda x: x[0]))
		#print r_dict
		#tracker_dict = OrderedDict('a':key,'b':"coming soon",'c':"coming soon")#,'d':round(question_dict[key]['first_factor_value'],3),'e':question_dict[key]['first_factor'],'f':question_dict[key]['second_factor'],'g':question_dict[key]['bucket_scheme'], 'h': round(question_dict[key]['rebucket_shares_1']*100),'i': question_dict[key]['rebucket_shares_2'],'j': question_dict[key]['rebucket_shares_3'],'k': round(question_dict[key]['best_rh']))
		tracker_dict = OrderedDict([('001_Q#',key),('002_Q_name','n/a'),('003_Dim','n/a'), ('004_PCA', round(question_dict[key]['first_factor_value'],3)),('005_#1 Factor', question_dict[key]['first_factor']),('006_#2 Factor', question_dict[key]['second_factor']),('007_Bucket', question_dict[key]['bucket_scheme']),('008_%T(=1)', round(question_dict[key]['rebucket_shares_1']*100)),('009_%M(=2)', round(question_dict[key]['rebucket_shares_2']*100)),('010_%L(=3)', round(question_dict[key]['rebucket_shares_3']*100)), ('011_RH',round(question_dict[key]['best_rh'],2))])
		#print tracker_dict
		tracker_dict.update(r_dict)
		pprint.pprint(tracker_dict, width=1)
		tracker_list.append(tracker_dict)
		#print tracker_list

	tracker = {'tracker': tracker_list}
	#print tracker

	return flask.jsonify(tracker)

# file upload section
# see http://code.runnable.com/UiPcaBXaxGNYAAAL/how-to-upload-a-file-to-the-server-in-flask-for-python
# https://github.com/moremorefor/flask-fileupload-ajax-example/blob/master/app.py
# http://stackoverflow.com/questions/18334717/how-to-upload-a-file-using-an-ajax-call-in-flask

# Route that will process the file upload
@app.route('/upload', methods=['POST'])
def upldfile():
    if request.method == 'POST':
        files = request.files['file']
        if files and allowed_file(files.filename):
            # # updir = '/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/uploads/'
            # updir = os.path.join(basedir_for_upload, 'uploads/')
            # files.save(os.path.join(updir, filename))
            # file_size = os.path.getsize(os.path.join(updir, filename))
            # return jsonify(name=filename, size=file_size)

            filename = secure_filename(files.filename)
            app.logger.info('FileName: ' + filename)
            updir = os.path.join(basedir_for_upload, 'uploads/')
            files.save(os.path.join(updir, filename))
            file_size = os.path.getsize(os.path.join(updir, filename))
            return jsonify(name=filename, size=file_size)

def scorecard(cluster_seeds, cluster_seed_names, num_seg):
	# step 7: load predicted clusters from file, add back to original data matrix (X)	
	# also calculate cluster scorecard metrics
	cluster_seeds, cluster_seed_names = cluster_seeds, cluster_seed_names

	# load datafile
	#basedir = '/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/'
	filename = 'predicted_segment.txt'

	#  http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.genfromtxt.html
	predicted_clusters = np.genfromtxt(basedir+filename, delimiter=',', skip_header=0)
	print filename, "loaded"
	print predicted_clusters.shape
	#print predicted_clusters[:100]
	#cluster_counts = Counter(predicted_clusters)
	#print cluster_counts # remember, this is dictionary-like object, e.g. cluster_counts[1] = number, etc

	filename = 'posterior_probabilities.txt'
	posterior_probabilities = np.genfromtxt(basedir+filename, delimiter=',', skip_header=0)

	filename = 'model_stats.txt'
	model_stats = np.genfromtxt(basedir+filename, delimiter=',', skip_header=0)

	print filename, "loaded"
	# calculate cluster scorecard metrics
	#print posterior_probabilities.shape
	#print posterior_probabilities[:100]
	cluster_size = [len(predicted_clusters[predicted_clusters == x+1]) for x in range(num_seg)]
	mean_posterior_probabilities = [np.mean(posterior_probabilities[:,x][predicted_clusters ==x+1]) for x in range(num_seg)] 
	cluster_shares = [float(cluster_size[x]) / sum(cluster_size) for x in range(num_seg)]

	# checksums
	if not sum(cluster_shares)==1:
		print "cluster shares do not sum to 1"
		print "cluster share sum:", sum(cluster_shares)

	if not sum(cluster_size)==predicted_clusters.shape[0]:
		print "cluster sizes do not sum to total"
		print "cluster size sum:", sum(cluster_size)
		print "total number of observations:", predicted_clusters.shape[0]

	# cluster scorecard
	print "-------  Cluster Scorecard  ---------"
	print "Timestamp:", time.asctime(time.localtime(start_time))
	print "Observations (n=):", len(predicted_clusters)
	print "Segments (n=):", num_seg
	print "Number of reps:", num_rep
	print "Clustering variables (n=):", len(cluster_seed_names)
	print "Clustering variables:", cluster_seed_names
	print "Model stats (MLE, Chi Sq, BIC):", ['%.2f' % (ms*1) for ms in list(model_stats)]
	print "Cluster size (n=):", cluster_size
	print "Cluster shares (%):", ['%.2f' % (cluster_share * 100) for cluster_share in cluster_shares]
	print "Mean posterior probabilities (%):", ['%.2f' % (mpp * 100) for mpp in mean_posterior_probabilities]
	print("------- Runtime: %.2f seconds -------" % (time.time() - start_time))

	return

def make_results_dict(results, X_rebucketed, names):
	
	# parse results data
	timestamps = [results[num][0] for num in range(len(results))]
	cluster_numbers = [results[num][3] for num in range(len(results))]
	num_vars = [len(results[num][2]) for num in range(len(results))]
	quest_list = [results[num][2] for num in range(len(results))]
	model_stats = [list(loadtxt(basedir+"/static/model/model_stats_"+timestamp+".txt", delimiter=",", unpack=False)) for timestamp in timestamps]
	predicted_clusters = [list(loadtxt(basedir+"/static/model/predicted_segment_"+timestamp+".txt", delimiter=",", unpack=False)) for timestamp in timestamps]
	posterior_probabilities = [list(loadtxt(basedir+"/static/model/posterior_probabilities_"+timestamp+".txt", delimiter=",", unpack=False)) for timestamp in timestamps]

	if len(set(timestamps)) != len(timestamps):
		print "warning: duplicate timestamps!"
	
	scorecard = zip(timestamps, cluster_numbers, num_vars, model_stats)
	#print sorted(scorecard, key=lambda x: x[3][2])[:10]
	
	# now store all in results_dict
	results_dict = {timestamp: {} for timestamp in timestamps}

	num = 0
	for key in results_dict.keys():
		results_dict[key]['run_number'] = num + 1
		results_dict[key]['cluster_number'] = cluster_numbers[num]
		results_dict[key]['cluster_counts'] = Counter(predicted_clusters[num]).values()
		results_dict[key]['cluster_shares'] = [ float(cluster) / sum(results_dict[key]['cluster_counts']) for cluster in results_dict[key]['cluster_counts']]
 		results_dict[key]['num_vars'] = num_vars[num]
		results_dict[key]['quest_list'] = quest_list[num]
		results_dict[key]['model_stats'] = model_stats[num]
		results_dict[key]['predicted_clusters'] = predicted_clusters[num]
		results_dict[key]['posterior_probabilities'] = posterior_probabilities[num]
		num +=1

	print "results_dict size:", len(results_dict)

	# within-segment polarity = ((max bucket share) - average bucket share) for each question and cluster
	# cross-question polarity = max bucket share - average bucket share for each question bucket across clusters
	
	print("------- Runtime: %.2f seconds -------" % (time.time() - start_time))
	
	results_dict = dict(Parallel(n_jobs=num_cores,verbose=5)(delayed(make_one_results_dict_entry_mp)(i,results_dict.keys()[i],results_dict[results_dict.keys()[i]], X_rebucketed, names) for i in range(len(results_dict.keys()))))
	#results_2_keys = results_dict.keys()
	#results_2_dict = dict(zip(results_2_keys, results_2))
	# could json.dumps be of help here? Send one results_dict[key] as results_dict[key].items() (=list), then convert to dict, and reverse / re-assemble on other end?
	# for key in results_dict:
	#   	make_one_results_dict_entry(1, key, results_dict, X_rebucketed, names)
		# pred_clusters = np.array(results_dict[key]['predicted_clusters'])
		# pred_clusters = np.reshape(pred_clusters, (pred_clusters.shape[0],1))
		# clustered_responders = np.append(X_rebucketed, pred_clusters, axis=1)
		
		# results_dict[key]['response_shares'] = {}; results_dict[key]['response_counts'] = {}; results_dict[key]['response_polarity'] = {}; results_dict[key]['cross_question_polarity'] = {}
		# questions = results_dict[key]['quest_list']
		
		# for question in results_dict[key]['quest_list']:
		# 	response_shares = []; response_counts = []
			
		# 	for cluster in set(results_dict[key]['predicted_clusters']):
		# 		response_count = [sum(clustered_responders[clustered_responders[:,clustered_responders.shape[1]-1] == cluster][:,names.index(question)] == bucket) for bucket in set(clustered_responders[:,names.index(question)])]
		# 		response_counts.append(response_count)
		# 		response_share = [float(response_count[i])/sum(response_count) for i in range(len(response_count))]
		# 		response_shares.append(response_share)

		# 	results_dict[key]['response_counts'][question] = response_counts
		# 	results_dict[key]['response_shares'][question] = response_shares
		# 	results_dict[key]['response_polarity'][question] = [max(item) - np.mean(item) for item in response_shares]
		# 	cross_question_polarity = np.reshape(results_dict[key]['response_shares'][question],(results_dict[key]['cluster_number'],3))
		# 	results_dict[key]['cross_question_polarity'][question] = [max(cross_question_polarity[:,col]) - np.mean(cross_question_polarity[:,col]) for col in range(cross_question_polarity.shape[1])]

	
		# results_dict[key]['polarity_scores'] = [sum(results_dict[key]['response_polarity'][question][cluster] / len (results_dict[key]['quest_list']) for question in results_dict[key]['quest_list']) for cluster in range(results_dict[key]['cluster_number'])]
		# results_dict[key]['weighted_average_cluster_polarity'] = sum([a*b for a,b in zip(results_dict[key]['polarity_scores'],results_dict[key]['cluster_shares'])])
		# results_dict[key]['mean_cluster polarity'] = np.mean(results_dict[key]['polarity_scores'])
		# results_dict[key]['average_cross_question_polarity'] = sum(sum(value) for value in results_dict[key]['cross_question_polarity'].values()) / len(results_dict[key]['cross_question_polarity'].values())	

		# print results_dict[key]['cluster_counts'], sum(results_dict[key]['cluster_counts'])
		# print results_dict[key]['cluster_shares'], sum(results_dict[key]['cluster_shares']) 
		# print 'polarity scores:', results_dict[key]['polarity_scores']
		# print "weighted average cluster polarity: ", results_dict[key]['weighted_average_cluster_polarity'] 
		# print "mean cluster polarity: ", results_dict[key]['mean_cluster polarity']
		# print "average cross_question_polarity: ", results_dict[key]['average_cross_question_polarity']
	
	#print "results_2 length:", len(results_2)
	#print "sample results_2 entry:", results_2[0]

	print "results_dict complete with", len(results_dict.keys()), "entries"
	print("------- Runtime: %.2f seconds -------" % (time.time() - start_time))

	
	return timestamps, results_dict

def make_one_results_dict_entry(i, key, results_dict, X_rebucketed, names):
	#results_dict_entry = dict(results_dict_entry)
	
	pred_clusters = np.array(results_dict[key]['predicted_clusters'])
	pred_clusters = np.reshape(pred_clusters, (pred_clusters.shape[0],1))
	clustered_responders = np.append(X_rebucketed, pred_clusters, axis=1)
	
	results_dict[key]['response_shares'] = {}; results_dict[key]['response_counts'] = {}; results_dict[key]['response_polarity'] = {}; results_dict[key]['cross_question_polarity'] = {}
	questions = results_dict[key]['quest_list']
	
	# for question in results_dict[key]['quest_list']:
	# for question in question_dict.keys():
	for question in names:
		response_shares = []; response_counts = []
		
		for cluster in set(results_dict[key]['predicted_clusters']):
			response_count = [sum(clustered_responders[clustered_responders[:,clustered_responders.shape[1]-1] == cluster][:,names.index(question)] == bucket) for bucket in set(clustered_responders[:,names.index(question)])]
			response_counts.append(response_count)
			response_share = [float(response_count[i])/sum(response_count) for i in range(len(response_count))]
			response_shares.append(response_share)

		results_dict[key]['response_counts'][question] = response_counts
		results_dict[key]['response_shares'][question] = response_shares
		results_dict[key]['response_polarity'][question] = [max(item) - np.mean(item) for item in response_shares]
		cross_question_polarity = np.reshape(results_dict[key]['response_shares'][question],(results_dict[key]['cluster_number'],3))
		results_dict[key]['cross_question_polarity'][question] = [max(cross_question_polarity[:,col]) - np.mean(cross_question_polarity[:,col]) for col in range(cross_question_polarity.shape[1])]


	results_dict[key]['polarity_scores'] = [sum(results_dict[key]['response_polarity'][question][cluster] / len (results_dict[key]['quest_list']) for question in results_dict[key]['quest_list']) for cluster in range(results_dict[key]['cluster_number'])]
	results_dict[key]['weighted_average_cluster_polarity'] = sum([a*b for a,b in zip(results_dict[key]['polarity_scores'],results_dict[key]['cluster_shares'])])
	results_dict[key]['mean_cluster polarity'] = np.mean(results_dict[key]['polarity_scores'])
	results_dict[key]['average_cross_question_polarity'] = sum(sum(value) for value in results_dict[key]['cross_question_polarity'].values()) / len(results_dict[key]['cross_question_polarity'].values())	

	print "number of entries for results_dict[key]", key, len(results_dict[key].keys())
	#print results_dict[key]
	one_entry = dict(results_dict[key].items())

	print "number of entries for one_entry", key, len(one_entry.keys())

	return one_entry

def make_one_results_dict_entry_mp(i, key, results_dict_entry, X_rebucketed, names):
	results_dict = dict(results_dict_entry)
	
	pred_clusters = np.array(results_dict['predicted_clusters'])
	pred_clusters = np.reshape(pred_clusters, (pred_clusters.shape[0],1))
	clustered_responders = np.append(X_rebucketed, pred_clusters, axis=1)
	
	results_dict['response_shares'] = {}; results_dict['response_counts'] = {}; results_dict['response_polarity'] = {}; results_dict['cross_question_polarity'] = {}
	questions = results_dict['quest_list']
	
	#for question in results_dict['quest_list']:
	# for question in question_dict.keys():
	for question in names:
		response_shares = []; response_counts = []
		
		for cluster in set(results_dict['predicted_clusters']):
			response_count = [sum(clustered_responders[clustered_responders[:,clustered_responders.shape[1]-1] == cluster][:,names.index(question)] == bucket) for bucket in set(clustered_responders[:,names.index(question)])]
			response_counts.append(response_count)
			response_share = [float(response_count[i])/sum(response_count) for i in range(len(response_count))]
			response_shares.append(response_share)

		#print question, "response counts:", response_counts	
		#print question, "response shares:", response_shares	
		results_dict['response_counts'][question] = response_counts
		results_dict['response_shares'][question] = response_shares
		results_dict['response_polarity'][question] = [max(item) - np.mean(item) for item in response_shares]
		cross_question_polarity = np.reshape(results_dict['response_shares'][question],(results_dict['cluster_number'],3))
		results_dict['cross_question_polarity'][question] = [max(cross_question_polarity[:,col]) - np.mean(cross_question_polarity[:,col]) for col in range(cross_question_polarity.shape[1])]

	print "results_dict[response_shares] length:", len(results_dict['response_shares'])
	
	results_dict['polarity_scores'] = [sum(results_dict['response_polarity'][question][cluster] / len (results_dict['quest_list']) for question in results_dict['quest_list']) for cluster in range(results_dict['cluster_number'])]
	results_dict['weighted_average_cluster_polarity'] = sum([a*b for a,b in zip(results_dict['polarity_scores'],results_dict['cluster_shares'])])
	results_dict['mean_cluster polarity'] = np.mean(results_dict['polarity_scores'])
	results_dict['average_cross_question_polarity'] = sum(sum(value) for value in results_dict['cross_question_polarity'].values()) / len(results_dict['cross_question_polarity'].values())	

	print "number of entries for results_dict", key, len(results_dict.keys())
	#print results_dict
	one_entry = dict(results_dict.items())

	print "number of entries for one_entry", key, len(one_entry.keys())

	return key, one_entry

def update_question_dict(question_dict, results_dict):
	print "number of keys in question_dict:", len(question_dict.keys())
	
	# initialize question_dict['run_tracker'] with null for all vars, since not all are used
	q = 0
	for key in question_dict:
		if 'run tracker' not in question_dict[key].keys(): 
			question_dict[key]['run_tracker'] = []
			question_dict[key]['bool_run_tracker'] = list(np.array([(run+1) in question_dict[key]['run_tracker'] for run in range(len(results_dict))]).astype(int))
		q = q + 1
	print q, "question_dict[key][run_tracker] initialized"

	# now update question_dict with run results
	for key in results_dict:
		for question in results_dict[key]['quest_list']:
			if 'run_tracker' in question_dict[question].keys():
				question_dict[question]['run_tracker'].append(results_dict[key]['run_number'])
			else:
				question_dict[question]['run_tracker'] = [results_dict[key]['run_number']]

			question_dict[question]['bool_run_tracker'] = list(np.array([(run+1) in question_dict[question]['run_tracker'] for run in range(len(results_dict))]).astype(int))

			#print question, question_dict[question]['run_tracker']
			#print question, question_dict[question]['bool_run_tracker']

	return question_dict, results_dict

def make_xls(results_dict):

	# uses http://xlsxwriter.readthedocs.org/getting_started.html
	# sample of results_dict[key]['response_shares']:
	#q16_5 response shares: [[0.36829268292682926, 0.4121951219512195, 0.21951219512195122], [0.2006172839506173, 0.41975308641975306, 0.37962962962962965], [0.2997032640949555, 0.5252225519287834, 0.17507418397626112], [0.09090909090909091, 0.5844155844155844, 0.3246753246753247], [0.07824427480916031, 0.7519083969465649, 0.16984732824427481]]

	outfile = 'clustering_data_export.xlsx'
	workbook = xlsxwriter.Workbook(basedir+outfile)
	run_number = 0

	for run in results_dict:
		run_number += 1
		worksheet_name = 'R' + str(run_number)
		worksheet = workbook.add_worksheet(worksheet_name)
		number_of_clusters = results_dict[run]['cluster_number'] 
		bold = workbook.add_format({'bold': True, 'align': 'center'})
		percent = workbook.add_format({'num_format': '0.00%', 'align': 'center'})

		row = 0
		col = 0

		worksheet.write(row, col + 0, 'question', bold)
		worksheet.write(row, col + 1, 'bucket', bold)

		for cluster in range(number_of_clusters):
			label = 'cluster ' + str(cluster+1)
			worksheet.write(row, col + cluster + 2, label,bold)

		row += 1

		for survey_question in results_dict[run]['response_shares'].keys():
			worksheet.write(row + 0, col + 0, survey_question, bold)
			worksheet.write(row + 0, col + 1, 'T=1', bold)
			worksheet.write(row + 1, col + 1, 'M=2', bold)
			worksheet.write(row + 2, col + 1, 'B=3', bold)

			for cluster in range(number_of_clusters):
				top, middle, bottom = results_dict[run]['response_shares'][survey_question][cluster]
				worksheet.write(row + 0, col + cluster + 2, top, percent)
				worksheet.write(row + 1, col + cluster + 2, top, percent)
				worksheet.write(row + 2, col + cluster + 2, top, percent)
			row += 4

		# for survey_question in results_dict[run]['response_shares'].keys():
		# 	row += 3
		# 	worksheet.write(row + 0, col + 0, survey_question, bold)
		# 	worksheet.write(row + 0, col + 1, 'T=1', bold)
	 #    	worksheet.write(row + 1, col + 1, 'M=2', bold)
	 #    	worksheet.write(row + 2, col + 1, 'B=3', bold)
	    	

	  #   	for cluster in range(number_of_clusters):
		 #    	col +=1
		 #    	top, middle, bottom = results_dict[run]['response_shares'][survey_question][cluster]
		 #    	worksheet.write(row + 0, col, top, percent)
		 #    	worksheet.write(row + 1, col, middle, percent)
		 #    	worksheet.write(row + 2, col, bottom, percent)
			# col = col - (number_of_clusters) - 1

	
	workbook.close()

	print "workbook exported: ", basedir+outfile
	return

if __name__ == "__main__":
	num_seg = 5
	num_rep = 1
	n_components = 30
	top_n = 2

	#basedir = '/Users/pniessen/Rosetta_Desktop/Segmentation_2-point-0/sample_case_work/GoPro/'
	filename = 'gopro_raw_data_v1.csv' 
	# file should be (user x question) matrix .csv with question labels in first row

	# data processing pipeline:
	factor_matrix, names, X, question_dict = get_PCA(filename, n_components)
	factor_matrix, num_rows, num_cols, question_number, rh, best_factor, second_best_factor, question_dict = top_n_factors(factor_matrix, top_n, question_dict)
	rebucketed_filename, X_rebucketed, question_dict = rebucket(factor_matrix, names, X, rh, question_dict)
	cluster_seed = cluster_seed(factor_matrix, best_factor, question_number, num_cols, num_rows)
	#cluster_seeds, cluster_seed_names, num_seg = poLCA (1, cluster_seed,num_seg,num_rep,rebucketed_filename)
	#scorecard(cluster_seeds, cluster_seed_names, num_seg)

	# run clustering in parallel if possible
	shortened_cluster_seeds = []
	num_remove = 10
	random.shuffle(cluster_seed)
	
	for num in range(num_remove):
	 	short_cluster_seed = cluster_seed[0:(len(cluster_seed)-num)]
		shortened_cluster_seeds.append(short_cluster_seed)

	# shortened_cluster_seeds = [x for x in itertools.combinations(cluster_seed, 28)]
	# print "total cluster seeds: ", len(shortened_cluster_seeds)

	num_seg = [5,6,7,8,9,10,11,12,13,14]
	
	num_cores = multiprocessing.cpu_count()
	#results = Parallel(n_jobs=num_cores,verbose=5)(delayed(poLCA)(i,cluster_seed, num_seg[i], num_rep, rebucketed_filename) for i in range(10))
	#scorecard(cluster_seeds, cluster_seed_names, num_seg)
	# see http://blog.dominodatalab.com/simple-parallelization/

	results = Parallel(n_jobs=num_cores,verbose=5)(delayed(poLCA)(i,shortened_cluster_seeds[j], num_seg[i], num_rep, rebucketed_filename) for i in range(len(num_seg)) for j in range (len(shortened_cluster_seeds)))
	print "number of runs completed:", len(results)

	# reporting pipeline
	timestamps, results_dict = make_results_dict(results, X_rebucketed, names)
	question_dict, results_dict = update_question_dict(question_dict,results_dict)
	
	print("------- Runtime: %.2f seconds -------" % (time.time() - start_time))

	# now clean up!

	for timestamp in timestamps:
		os.remove(basedir+"/static/model/model_stats_"+timestamp+".txt")
		os.remove(basedir+"/static/model/predicted_segment_"+timestamp+".txt")
		os.remove(basedir+"/static/model/posterior_probabilities_"+timestamp+".txt")
	print len(timestamps) * 3, "scoring files cleaned up from ", len(timestamps), "runs"

	if xls:
		make_xls(results_dict)

	if interactive_mode:
		# app.run()
		app.run(host='0.0.0.0', port=80)

	# issue backlog:
	# (x) handoff from Python > R
	# (x)	specify which variables to use in poLCA clustering
	# (x) parse return data from PoLCA - predicted clusters for each observation?
	# (x) remove col 32-35 reference, develop "question_number", "best_factor", "second_best_factor", "rh" labels
	# (x) send parameters to poLCA: number of clusters, number of iterations, etc
	# (x) add predicted posterior to return data, calculate cluster size counts and mean predicted posterior for each cluster
	# (c) i/o to same directory
	# (c) add AIC, BIC to model scorecard
	# (x) bucketing / formatting for poLCA custom input
	# (x) test different number of factors (!= 32)
	# (x) web front end beta
	# (x) reorder run_tracker
	# (x) add switches to control panel
	# (x) migrate start to top nav bar
	# (x) reorder run_tracker
	# (x) add switches to control panel
	# (x) migrate start to top nav bar
	# (x) multiprocessing for result_dictionary creation?
	# (x) user option: save .xls files
	# (x) broaden gridsearch with combinations (Y take X)
	# (x) drag / drop question chooser (with "rewind" feature to run number X) ("this run is x% similar")
	# control panel: option for other methods, knn, DBSCAN, tensorflow
	# control panel: small vs large gridsearch
	# 2-d plot of runs - cohesive (X) vs differentiation (Y)?
	# timestamp resuls_dict and question_dict for web service to multiple cases?
	# visualization - 3D (users, PCA(questions)(:3), clusters)
	# Cython for frequently used modules?
	# data cleanup
	# data weighting
	# how to rebucket binary and continuous data (vs 1-7)
	# audit trail - save X, X_rebucketed, factor_matrix, run_tracker, results_tracker to disk?
	# visual - radial graph with nearest neighbors?
	# recommendation - top X%?
